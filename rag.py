"""
###############################################################################
¬øQU√â ES UN SISTEMA RAG?
------------------------------------------------------------------------------
RAG = Retrieval-Augmented Generation (Generaci√≥n Aumentada con Recuperaci√≥n).
Consiste en dos fases:
1. Recuperaci√≥n: encontrar los fragmentos m√°s relevantes dentro de tus
   documentos para una pregunta dada.
2. Generaci√≥n: un LLM crea la respuesta final a partir de esos fragmentos,
   reduciendo al m√≠nimo las ‚Äúalucinaciones‚Äù.

FLUJO CONCEPTUAL
----------------
Usuario ‚Üí Pregunta
        ‚Üì
Retriever (busca similitud en vectores)
        ‚Üì
LLM + Prompt con contexto ‚Üí Respuesta
------------------------------------------------------------------------------
En este script todo ocurre localmente con:
- PyPDFLoader      ‚Üí lee PDFs
- RecursiveCharacterTextSplitter ‚Üí trocea el texto
- Chroma           ‚Üí base vectorial
- all-minilm       ‚Üí modelo de embeddings
- Llama 3 (Ollama) ‚Üí LLM
###############################################################################
"""

# -------------------------------------------------------------------
# 1. IMPORTACIONES
#    Cada m√≥dulo cumple un papel espec√≠fico dentro del pipeline RAG.
# -------------------------------------------------------------------
from langchain_community.document_loaders import PyPDFLoader       # Lee PDF
from langchain.text_splitter import RecursiveCharacterTextSplitter # Trocea texto
from langchain_chroma import Chroma                                # BD vectorial
from langchain_ollama import OllamaEmbeddings, ChatOllama          # Embeds + LLM
from langchain_core.prompts import ChatPromptTemplate              # Prompt template
from langchain_core.runnables import RunnablePassthrough           # Passthrough
from langchain_core.output_parsers import StrOutputParser          # Parseo a str

# -------------------------------------------------------------------
# 2. CARGAR EL DOCUMENTO
#    PyPDFLoader convierte cada p√°gina del PDF en un objeto Document
#    con atributos: page_content (str) y metadata (dict).
# -------------------------------------------------------------------
loader = PyPDFLoader("docs/test.pdf")
docs = loader.load()  # Lista de Document, 1 por p√°gina

# -------------------------------------------------------------------
# 3. DIVIDIR EN FRAGMENTOS (CHUNKS)
#    Teor√≠a de los chunks:
#    - chunk_size   ‚âà n¬∫ de caracteres por trozo.
#    - chunk_overlap= cu√°ntos caracteres se repiten entre trozos
#                     para conservar contexto.
#    Objetivo: que cada trozo quepa en el contexto del LLM y a la vez
#              mantenga coherencia sem√°ntica.
# -------------------------------------------------------------------
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,   # ~1000 caracteres por chunk
    chunk_overlap=200  # 200 caracteres de solapamiento
)
splits = text_splitter.split_documents(docs)  # Lista m√°s grande de trozos peque√±os

# -------------------------------------------------------------------
# 4. CREAR EMBEDDINGS Y VECTORSTORE
#    - Embeddings: vectores densos que representan el significado
#      sem√°ntico de cada chunk (modelo all-minilm).
#    - Chroma: base de datos vectorial que indexa esos vectores y
#      permite b√∫squeda por similitud coseno.
# -------------------------------------------------------------------
embeddings = OllamaEmbeddings(model="all-minilm")  # Modelo local de Ollama
vectorstore = Chroma.from_documents(
    documents=splits,      # Todos los trozos
    embedding=embeddings   # Funci√≥n para convertir texto ‚Üí vector
)

# -------------------------------------------------------------------
# 5. RECUPERADOR
#    Objeto que, dada una pregunta, retorna los k trozos m√°s similares
#    (por defecto k=4). Internamente usa la similitud coseno entre
#    el embedding de la pregunta y los embeddings de los trozos.
# -------------------------------------------------------------------
retriever = vectorstore.as_retriever()  # k=4 por defecto

# -------------------------------------------------------------------
# 6. MODELO LLM
#    Llama 3 ejecutado localmente v√≠a Ollama. Genera la respuesta
#    final a partir del prompt completo.
# -------------------------------------------------------------------
llm = ChatOllama(model="llama3")

# -------------------------------------------------------------------
# 7. PLANTILLA DE PROMPT
#    Define c√≥mo se ensambla el prompt final:
#    - {context}  : trozos recuperados (formateados en texto plano)
#    - {question} : pregunta del usuario
# -------------------------------------------------------------------
prompt = ChatPromptTemplate.from_template(
    """Eres un asistente √∫til. Usa solo el contexto para responder.

Contexto: {context}

Pregunta: {question}

Respuesta:"""
)

# -------------------------------------------------------------------
# 8. FUNCI√ìN AUXILIAR
#    Une los trozos recuperados en un √∫nico string para el prompt.
# -------------------------------------------------------------------
def format_docs(docs):
    """
    Recibe una lista de Document y devuelve su contenido concatenado.
    """
    return "\n\n".join(doc.page_content for doc in docs)

# -------------------------------------------------------------------
# 9. CADENA RAG
#    Pipeline declarativo de LangChain:
#    - Paso 1: recibe la pregunta.
#    - Paso 2: retriever obtiene trozos relevantes.
#    - Paso 3: format_docs une los trozos.
#    - Paso 4: se inyectan en el prompt.
#    - Paso 5: Llama 3 genera la respuesta.
#    - Paso 6: se parsea a string plano.
# -------------------------------------------------------------------
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# -------------------------------------------------------------------
# 10. BUCLE INTERACTIVO
#    Permite probar la cadena en consola.
# -------------------------------------------------------------------
if __name__ == "__main__":
    print("üîç RAG activo. Escribe 'salir' para terminar.\n")
    while True:
        query = input("Pregunta: ")
        if query.lower() == "salir":
            break
        respuesta = rag_chain.invoke(query)
        print("Respuesta:", respuesta, "\n")